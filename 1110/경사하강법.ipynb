{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oW_dSbELr4kP",
        "outputId": "10af2546-73bc-4e93-f8a7-f586dd6f420e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0번째 loss : 39.80526573375\n",
            "0번째 w0, w1 : 0.012050000000000002, 0.03505 \n",
            "\n",
            "100번째 loss : 1.4088818550020328\n",
            "100번째 w0, w1 : 0.5956839328965645, 1.7137245928510965 \n",
            "\n",
            "200번째 loss : 0.08492622860667261\n",
            "200번째 w0, w1 : 0.7129266308916146, 2.0222518370363343 \n",
            "\n",
            "300번째 loss : 0.03808809478420731\n",
            "300번째 w0, w1 : 0.7433441610180002, 2.0765698140220747 \n",
            "\n",
            "400번째 loss : 0.03531467697283942\n",
            "400번째 w0, w1 : 0.7573915882750667, 2.0837935212742202 \n",
            "\n",
            "500번째 loss : 0.03412659366543372\n",
            "500번째 w0, w1 : 0.7681528769083021, 2.0823608526346415 \n",
            "\n",
            "600번째 loss : 0.033056629416450725\n",
            "600번째 w0, w1 : 0.7780637167868475, 2.079403378165377 \n",
            "\n",
            "700번째 loss : 0.032050551054706465\n",
            "700번째 w0, w1 : 0.787583266851497, 2.0762423007849082 \n",
            "\n",
            "800번째 loss : 0.031103013292289844\n",
            "800번째 w0, w1 : 0.7968036334081118, 2.0731204929145344 \n",
            "\n",
            "900번째 loss : 0.030210557013104773\n",
            "900번째 w0, w1 : 0.8057485947959019, 2.070080758386106 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 사용할 1차 선형 회귀 모델\n",
        "\n",
        "def linear_model(w0, w1, X):\n",
        "\n",
        "    f_x = w0 + w1 * X\n",
        "\n",
        "    return f_x\n",
        "\n",
        "'''\n",
        "1. 설명 중 '손실 함수' 파트의 수식을 참고해\n",
        "   MSE 손실 함수를 완성하세요.\n",
        "'''\n",
        "\n",
        "def Loss(f_x, y): #f_x :예측값, y:정답\n",
        "\n",
        "    ls = np.mean(np.square(y - f_x)) #정답값과 예측값값의 차이\n",
        "\n",
        "    return ls\n",
        "\n",
        "'''\n",
        "2. 설명 중 'Gradient' 파트의 마지막 두 수식을 참고해 두 가중치\n",
        "   w0와 w1에 대한 gradient인 'gradient0'와 'gradient1'을\n",
        "   반환하는 함수 gradient_descent 함수를 완성하세요.\n",
        "\n",
        "   Step01. w0에 대한 gradient인 'gradient0'를 작성합니다.\n",
        "\n",
        "   Step02. w1에 대한 gradient인 'gradient1'을 작성합니다.\n",
        "'''\n",
        "\n",
        "def gradient_descent(w0, w1, X, y):\n",
        "\n",
        "    gradient0 = 2 * np.mean((y - (w0+w1 *X)) * (-1))\n",
        "    gradient1 = 2 * np.mean((y - (w0 + w1 * X)) * (-1 *X))\n",
        "\n",
        "    return np.array([gradient0, gradient1])\n",
        "\n",
        "'''\n",
        "3. 설명 중 '가중치 업데이트' 파트의 두 수식을 참고해\n",
        "   gradient descent를 통한 가중치 업데이트 코드를 작성하세요.\n",
        "\n",
        "   Step01. 앞서 완성한 gradient_descent 함수를 이용해\n",
        "           w0와 w1에 대한 gradient인 'gd'를 정의하세요.\n",
        "\n",
        "   Step02. 변수 'w0'와 'w1'에 두 가중치 w0와 w1을\n",
        "           업데이트하는 코드를 작성합니다. 앞서 정의한\n",
        "           변수 'gd'와 이미 정의된 변수 'lr'을 사용하세요.\n",
        "'''\n",
        "\n",
        "def main():\n",
        "\n",
        "    X = np.array([1,2,3,4]).reshape((-1,1))\n",
        "    y = np.array([3.1, 4.9, 7.2, 8.9]).reshape((-1,1))\n",
        "\n",
        "    # 파라미터 초기화\n",
        "    w0 = 0\n",
        "    w1 = 0\n",
        "\n",
        "    # learning rate 설정\n",
        "    lr = 0.001\n",
        "\n",
        "    # 반복 횟수 1000으로 설정\n",
        "    for i in range(1000):\n",
        "\n",
        "        gd = gradient_descent(w0, w1, X, y)\n",
        "\n",
        "        w0 = w0 - lr * gd[0]\n",
        "        w1 = w1 - lr * gd[1]\n",
        "\n",
        "        # 100회마다의 해당 loss와 w0, w1 출력\n",
        "        if (i % 100 == 0):\n",
        "\n",
        "            loss = Loss(linear_model(w0,w1,X),y)\n",
        "\n",
        "            print(\"{}번째 loss : {}\".format(i, loss))\n",
        "            print(\"{}번째 w0, w1 : {}, {}\".format(i, w0, w1),'\\n')\n",
        "\n",
        "    return w0, w1\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x =[2,4,6,8] #공부한 시간\n",
        "y =[81,93,91,97] #점수\n",
        "\n",
        "#x와 y의 평균\n",
        "\n",
        "mx = np.mean(x)\n",
        "my = np.mean(y)\n",
        "\n",
        "print(\"x의 평균값:\", x)\n",
        "print(\"y의 평균값:\", y)\n",
        "\n",
        "#기울기 공식 분모 :x의 평균값과 x의 각 원소들의 차를 제곱\n",
        "divisor = sum([(mx - i)**2 for i in x])\n",
        "\n",
        "#기울기 공식의 분자 y값의 증가율\n",
        "def top(x, mx, y, my):\n",
        "  d = 0\n",
        "  for i in range(len(x)):\n",
        "    d +=(x[i] -mx) * (y[i] - my)\n",
        "  return d\n",
        "divided = top(x, mx, y, my)\n",
        "\n",
        "print(\"분모:\", divisor)\n",
        "print(\"분자:\", divided)\n",
        "\n",
        "\n",
        "#기울기와 y 절편 구하기\n",
        "\n",
        "a = divided/divisor\n",
        "b= my - (mx *a)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH5usliWyleV",
        "outputId": "86c8bb3e-e479-4ace-f74f-d8b4f2397c76"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x의 평균값: [2, 4, 6, 8]\n",
            "y의 평균값: [81, 93, 91, 97]\n",
            "분모: 20.0\n",
            "분자: 46.0\n"
          ]
        }
      ]
    }
  ]
}